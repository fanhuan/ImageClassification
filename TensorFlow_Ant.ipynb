{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "from skimage import color, io\n",
    "from scipy.misc import imresize\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import tflearn\n",
    "from tflearn.data_utils import shuffle, to_categorical\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.data_preprocessing import ImagePreprocessing\n",
    "from tflearn.data_augmentation import ImageAugmentation\n",
    "from tflearn.metrics import Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3672\n"
     ]
    }
   ],
   "source": [
    "files_path = '/home/hfan/projects/Heidi/sub_train_ant/'\n",
    "\n",
    "yes_files_path = os.path.join(files_path, 'yes_*.png')\n",
    "no_files_path = os.path.join(files_path, 'no_*.png')\n",
    "\n",
    "yes_files = sorted(glob(yes_files_path))\n",
    "no_files = sorted(glob(no_files_path))\n",
    "\n",
    "n_files = len(yes_files) + len(no_files)\n",
    "print(n_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1126 2546\n"
     ]
    }
   ],
   "source": [
    "print(len(yes_files),len(no_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size_image = 64\n",
    "allX = np.zeros((n_files, size_image, size_image, 3), dtype='float64')\n",
    "ally = np.zeros(n_files)\n",
    "count = 0\n",
    "for f in yes_files:\n",
    "    try:\n",
    "        img = io.imread(f)\n",
    "        new_img = imresize(img, (size_image, size_image, 3))\n",
    "        allX[count] = np.array(new_img)\n",
    "        ally[count] = 0\n",
    "        count += 1\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "for f in no_files:\n",
    "    try:\n",
    "        img = io.imread(f)\n",
    "        new_img = imresize(img, (size_image, size_image, 3))\n",
    "        allX[count] = np.array(new_img)\n",
    "        ally[count] = 1\n",
    "        count += 1\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test-train split   \n",
    "X, X_test, Y, Y_test = train_test_split(allX, ally, test_size=0.1, random_state=42)\n",
    "\n",
    "# encode the Ys\n",
    "Y = to_categorical(Y, 2)\n",
    "Y_test = to_categorical(Y_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalisation of images\n",
    "img_prep = ImagePreprocessing()\n",
    "img_prep.add_featurewise_zero_center()\n",
    "img_prep.add_featurewise_stdnorm()\n",
    "\n",
    "# Create extra synthetic training data by flipping & rotating images\n",
    "img_aug = ImageAugmentation()\n",
    "img_aug.add_random_flip_leftright()\n",
    "img_aug.add_random_rotation(max_angle=25.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input is a 32x32 image with 3 color channels (red, green and blue)\n",
    "network = input_data(shape=[None, 64, 64, 3],\n",
    "                     data_preprocessing=img_prep, \n",
    "                     data_augmentation=img_aug)\n",
    "\n",
    "# 1: Convolution layer with 32 filters, each 3x3x3\n",
    "conv_1 = conv_2d(network, 32, 3, activation='relu', name='conv_1')\n",
    "\n",
    "# 2: Max pooling layer\n",
    "network = max_pool_2d(conv_1, 2)\n",
    "\n",
    "# 3: Convolution layer with 64 filters\n",
    "conv_2 = conv_2d(network, 64, 3, activation='relu', name='conv_2')\n",
    "\n",
    "# 4: Convolution layer with 64 filters\n",
    "conv_3 = conv_2d(conv_2, 64, 3, activation='relu', name='conv_3')\n",
    "\n",
    "# 5: Max pooling layer\n",
    "network = max_pool_2d(conv_3, 2)\n",
    "\n",
    "# 6: Fully-connected 512 node layer\n",
    "network = fully_connected(network, 512, activation='relu')\n",
    "\n",
    "# 7: Dropout layer to combat overfitting\n",
    "network = dropout(network, 0.5)\n",
    "\n",
    "# 8: Fully-connected layer with two outputs\n",
    "network = fully_connected(network, 2, activation='softmax')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configure how the network will be trained\n",
    "acc = Accuracy(name=\"Accuracy\")\n",
    "network = regression(network, optimizer='adam',\n",
    "                     loss='categorical_crossentropy',\n",
    "                     learning_rate=0.0005, metric=acc)\n",
    "\n",
    "# Wrap the network in a model object\n",
    "model = tflearn.DNN(network, checkpoint_path='model_cat_dog_6.tflearn', max_checkpoints = 3,\n",
    "                    tensorboard_verbose = 3, tensorboard_dir='tmp/tflearn_logs/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 669  | total loss: \u001b[1m\u001b[32m0.38120\u001b[0m\u001b[0m | time: 50.625s\n",
      "| Adam | epoch: 010 | loss: 0.38120 - Accuracy: 0.8312 -- iter: 3300/3304\n",
      "Training Step: 670  | total loss: \u001b[1m\u001b[32m0.37407\u001b[0m\u001b[0m | time: 52.403s\n",
      "| Adam | epoch: 010 | loss: 0.37407 - Accuracy: 0.8361 | val_loss: 0.41740 - val_acc: 0.8234 -- iter: 3304/3304\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "# Train model for 10 epochs\n",
    "###################################\n",
    "model.fit(X, Y, validation_set=(X_test, Y_test), batch_size=50,\n",
    "      n_epoch=10, run_id='model_cat_dog_6', show_metric=True)\n",
    "\n",
    "model.save('model_sub_ant_20170809_batch50_epoch10.tflearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('model_sub_ant_20170809_batch50_epoch10.tflearn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_path = '/home/hfan/projects/Heidi/train_ant/yes_*_ants_*_1*.png'\n",
    "\n",
    "files = sorted(glob(file_path))\n",
    "\n",
    "size_image = 64\n",
    "\n",
    "predictX = np.zeros((10, size_image, size_image, 3), dtype='float64')\n",
    "\n",
    "count = 0\n",
    "while count < 10:\n",
    "    for f in files:\n",
    "        try:\n",
    "            img = io.imread(f)\n",
    "            new_img = imresize(img, (size_image, size_image, 3))\n",
    "            predictX[count] = np.array(new_img)\n",
    "            count += 1\n",
    "        except:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = model.predict(predictX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.52945817,  0.47054178],\n",
       "       [ 0.66306144,  0.3369385 ],\n",
       "       [ 0.46866161,  0.53133839],\n",
       "       [ 0.42004302,  0.57995701],\n",
       "       [ 0.57061899,  0.42938098],\n",
       "       [ 0.55663311,  0.44336686],\n",
       "       [ 0.22094165,  0.77905834],\n",
       "       [ 0.40128914,  0.59871083],\n",
       "       [ 0.66350681,  0.33649316],\n",
       "       [ 0.13621238,  0.86378765]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/hfan/projects/Heidi/train_ant/yes_09MAY2016_ants_10_1.png',\n",
       " '/home/hfan/projects/Heidi/train_ant/yes_09MAY2016_ants_10_10.png',\n",
       " '/home/hfan/projects/Heidi/train_ant/yes_09MAY2016_ants_10_100.png',\n",
       " '/home/hfan/projects/Heidi/train_ant/yes_09MAY2016_ants_10_101.png',\n",
       " '/home/hfan/projects/Heidi/train_ant/yes_09MAY2016_ants_10_102.png',\n",
       " '/home/hfan/projects/Heidi/train_ant/yes_09MAY2016_ants_10_103.png',\n",
       " '/home/hfan/projects/Heidi/train_ant/yes_09MAY2016_ants_10_104.png',\n",
       " '/home/hfan/projects/Heidi/train_ant/yes_09MAY2016_ants_10_105.png',\n",
       " '/home/hfan/projects/Heidi/train_ant/yes_09MAY2016_ants_10_11.png',\n",
       " '/home/hfan/projects/Heidi/train_ant/yes_09MAY2016_ants_10_110.png']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(results[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.529458\n"
     ]
    }
   ],
   "source": [
    "print(np.amax(results[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
